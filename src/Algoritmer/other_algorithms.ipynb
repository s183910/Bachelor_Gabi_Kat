{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://quantdare.com/learning-to-rank-with-tensorflow/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\r\n",
    "import tensorflow_ranking as tfr\r\n",
    "\r\n",
    "_TRAIN_DATA_PATH=\"/data/train.txt\"\r\n",
    "_TEST_DATA_PATH=\"/data/test.txt\"\r\n",
    "_LOSS=\"approx_ndcg_loss\"\r\n",
    "_N_ASSETS=100 # hvad er denne?\r\n",
    "_N_FEATURES=16 # er det her hvor vi blot har 3 features?\r\n",
    "_BATCH_SIZE=32\r\n",
    "_HIDDEN_LAYER_DIMS=[\"20\", \"10\"]\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\r\n",
    "# Input Pipeline\r\n",
    "def input_fn(path):\r\n",
    "    data = tf.data.Dataset.from_generator(\r\n",
    "        tfr.data.libsvm_generator(path, _N_FEATURES, _N_ASSETS),\r\n",
    "        output_types=({str(k): tf.float32 for k in range(1,_N_FEATURES+1)}, tf.float32),\r\n",
    "        output_shapes=(\r\n",
    "            {str(k): tf.TensorShape([_N_ASSETS, 1]) for k in range(1,_N_FEATURES+1)},\r\n",
    "            tf.TensorShape([_N_ASSETS])\r\n",
    "        )\r\n",
    "    )\r\n",
    "\r\n",
    "    data = data.shuffle(1000).repeat().batch(_BATCH_SIZE)\r\n",
    "  \r\n",
    "    return data.make_one_shot_iterator().get_next()\r\n",
    "\r\n",
    "\r\n",
    "def example_feature_columns():\r\n",
    "    \"\"\"Returns the example feature columns.\"\"\"\r\n",
    "    \r\n",
    "    feature_names = [\"%d\" % (i + 1) for i in range(0, _N_FEATURES)]\r\n",
    "    \r\n",
    "    return {name: tf.feature_column.numeric_column(name, shape=(1,), default_value=0.0)\r\n",
    "            for name in feature_names}\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\r\n",
    "# Scoring Function\r\n",
    "def make_score_fn():\r\n",
    "    \"\"\"Returns a scoring function to build `EstimatorSpec`.\"\"\"\r\n",
    "\r\n",
    "    def _score_fn(context_features, group_features, mode, params, config):\r\n",
    "        \"\"\"Defines the network to score assets.\"\"\"\r\n",
    "        \r\n",
    "        del params\r\n",
    "        del config\r\n",
    "        # Define input layer.\r\n",
    "        example_input = [\r\n",
    "            tf.layers.flatten(group_features[name])\r\n",
    "            for name in sorted(example_feature_columns())\r\n",
    "        ]\r\n",
    "        input_layer = tf.concat(example_input, 1)\r\n",
    "\r\n",
    "        cur_layer = input_layer\r\n",
    "        for i, layer_width in enumerate(int(d) for d in _HIDDEN_LAYER_DIMS):\r\n",
    "            cur_layer = tf.layers.dense(\r\n",
    "                cur_layer,\r\n",
    "                units=layer_width,\r\n",
    "                activation=\"tanh\")\r\n",
    "\r\n",
    "        logits = tf.layers.dense(cur_layer, units=1)\r\n",
    "        \r\n",
    "        return logits\r\n",
    "\r\n",
    "    return _score_fn\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\r\n",
    "\r\n",
    "# Evaluation Metric\r\n",
    "def eval_metric_fns():\r\n",
    "      \"\"\"Returns a dict from name to metric functions.\r\n",
    "      Returns:\r\n",
    "        A dict mapping from metric name to a metric function with above signature.\r\n",
    "      \"\"\"\r\n",
    "      \r\n",
    "      metric_fns = {}\r\n",
    "      metric_fns.update({\r\n",
    "        \"metric/ndcg@%d\" % topn: tfr.metrics.make_ranking_metric_fn(\r\n",
    "            tfr.metrics.RankingMetricKey.NDCG, topn=topn)\r\n",
    "        for topn in [1, 3, 5, 10]\r\n",
    "    })\r\n",
    "    \r\n",
    "      return metric_fns\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-8-e2be0218d8be>, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-8-e2be0218d8be>\"\u001b[1;36m, line \u001b[1;32m12\u001b[0m\n\u001b[1;33m    return tf.contrib.layers.optimize_loss(\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "\r\n",
    "# Estimator\r\n",
    "def get_estimator(hparams):\r\n",
    "    \"\"\"Create a ranking estimator.\r\n",
    "    Args:\r\n",
    "    hparams: (tf.contrib.training.HParams) a hyperparameters object.\r\n",
    "    Returns:\r\n",
    "    tf.learn `Estimator`.\r\n",
    "    \"\"\"\r\n",
    "    \r\n",
    "    def _train_op_fn(loss):\r\n",
    "    # \"\"\"Defines train op used in ranking head.\"\"\"\r\n",
    "    return tf.contrib.layers.optimize_loss(\r\n",
    "        loss=loss,\r\n",
    "        global_step=tf.train.get_global_step(),\r\n",
    "        learning_rate=hparams.learning_rate,\r\n",
    "        optimizer=\"Adagrad\")\r\n",
    "\r\n",
    "    ranking_head = tfr.head.create_ranking_head(\r\n",
    "        loss_fn=tfr.losses.make_loss_fn(_LOSS),\r\n",
    "        eval_metric_fns=eval_metric_fns(),\r\n",
    "        train_op_fn=_train_op_fn)\r\n",
    "\r\n",
    "    return tf.estimator.Estimator(\r\n",
    "        model_fn=tfr.model.make_groupwise_ranking_fn(\r\n",
    "          group_score_fn=make_score_fn(),\r\n",
    "          group_size=1,\r\n",
    "          transform_fn=None,\r\n",
    "          ranking_head=ranking_head),\r\n",
    "        params=hparams)\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\r\n",
    "# Initialize estimator\r\n",
    "hparams = tf.contrib.training.HParams(learning_rate=0.05)\r\n",
    "ranker = get_estimator(hparams)\r\n",
    "\r\n",
    "# Train model\r\n",
    "ranker.train(input_fn=lambda: input_fn(_TRAIN_DATA_PATH), steps=100)\r\n",
    "\r\n",
    "# Evaluate model\r\n",
    "ranker.evaluate(input_fn=lambda: input_fn(_TEST_DATA_PATH), steps=100)\r\n",
    "\r\n",
    "# Visualize\r\n",
    "ranker.model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalFlagValueError",
     "evalue": "flag --train_path=None: Flag --train_path must have a value other than None.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\absl\\flags\\_flagvalues.py\u001b[0m in \u001b[0;36m_assert_validators\u001b[1;34m(self, validators)\u001b[0m\n\u001b[0;32m    549\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m         \u001b[0mvalidator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0m_exceptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mValidationError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\absl\\flags\\_validators.py\u001b[0m in \u001b[0;36mverify\u001b[1;34m(self, flag_values)\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchecker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0m_exceptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mValidationError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValidationError\u001b[0m: Flag --train_path must have a value other than None.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mIllegalFlagValueError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-8ec5756849b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    332\u001b[0m   \u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmark_flag_as_required\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"eval_path\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m   \u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmark_flag_as_required\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"model_dir\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 334\u001b[1;33m   \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(main, argv)\u001b[0m\n\u001b[0;32m     38\u001b[0m   \u001b[0mmain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmain\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'__main__'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m   \u001b[0m_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags_parser\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_parse_flags_tolerate_undef\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\absl\\app.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(main, argv, flags_parser)\u001b[0m\n\u001b[0;32m    293\u001b[0m   \"\"\"\n\u001b[0;32m    294\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m     args = _run_init(\n\u001b[0m\u001b[0;32m    296\u001b[0m         \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0margv\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0margv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m         \u001b[0mflags_parser\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\absl\\app.py\u001b[0m in \u001b[0;36m_run_init\u001b[1;34m(argv, flags_parser)\u001b[0m\n\u001b[0;32m    362\u001b[0m   \u001b[1;31m# Set up absl logging handler.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m   \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_absl_handler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 364\u001b[1;33m   args = _register_and_parse_flags_with_usage(\n\u001b[0m\u001b[0;32m    365\u001b[0m       \u001b[0margv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m       \u001b[0mflags_parser\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflags_parser\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\absl\\app.py\u001b[0m in \u001b[0;36m_register_and_parse_flags_with_usage\u001b[1;34m(argv, flags_parser)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m   \u001b[0moriginal_argv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0margv\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0margv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m   \u001b[0margs_to_main\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mflags_parser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal_argv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_parsed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'FLAGS must be parsed after flags_parser is called.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\u001b[0m in \u001b[0;36m_parse_flags_tolerate_undef\u001b[1;34m(argv)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_parse_flags_tolerate_undef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m   \u001b[1;34m\"\"\"Parse args, returning any unknown flags (ABSL defaults to crashing).\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0margv\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0margv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mknown_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\platform\\flags.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'__wrapped'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\absl\\flags\\_flagvalues.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, argv, known_only)\u001b[0m\n\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmark_as_parsed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 658\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidate_all_flags\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    659\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mprogram_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0munparsed_args\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\absl\\flags\\_flagvalues.py\u001b[0m in \u001b[0;36mvalidate_all_flags\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mflag\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitervalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flags\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m       \u001b[0mall_validators\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidators\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_assert_validators\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_validators\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_assert_validators\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidators\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\absl\\flags\\_flagvalues.py\u001b[0m in \u001b[0;36m_assert_validators\u001b[1;34m(self, validators)\u001b[0m\n\u001b[0;32m    551\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0m_exceptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mValidationError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_flags_with_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 553\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0m_exceptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIllegalFlagValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%s: %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    554\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__delattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflag_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIllegalFlagValueError\u001b[0m: flag --train_path=None: Flag --train_path must have a value other than None."
     ]
    }
   ],
   "source": [
    "\r\n",
    "from absl import flags\r\n",
    "\r\n",
    "import tensorflow as tf\r\n",
    "import tensorflow_ranking as tfr\r\n",
    "\r\n",
    "flags.DEFINE_enum(\r\n",
    "    \"data_format\", \"example_list_with_context\",\r\n",
    "    [\"example_list_with_context\", \"example_in_example\", \"sequence_example\"],\r\n",
    "    \"Data format defined in data.py.\")\r\n",
    "flags.DEFINE_string(\"train_path\", None, \"Input file path used for training.\")\r\n",
    "flags.DEFINE_string(\"eval_path\", None, \"Input file path used for eval.\")\r\n",
    "flags.DEFINE_string(\"vocab_path\", None,\r\n",
    "                    \"Vocabulary path for query and document tokens.\")\r\n",
    "flags.DEFINE_string(\"model_dir\", None, \"Output directory for models.\")\r\n",
    "flags.DEFINE_integer(\"batch_size\", 32, \"The batch size for train.\")\r\n",
    "flags.DEFINE_integer(\"num_train_steps\", 15000, \"Number of steps for train.\")\r\n",
    "flags.DEFINE_float(\"learning_rate\", 0.05, \"Learning rate for optimizer.\")\r\n",
    "flags.DEFINE_float(\"dropout_rate\", 0.8, \"The dropout rate before output layer.\")\r\n",
    "flags.DEFINE_list(\"hidden_layer_dims\", [\"64\", \"32\", \"16\"],\r\n",
    "                  \"Sizes for hidden layers.\")\r\n",
    "flags.DEFINE_integer(\r\n",
    "    \"list_size\", None,\r\n",
    "    \"List size used for training. Use None for dynamic list size.\")\r\n",
    "flags.DEFINE_integer(\"group_size\", 1, \"Group size used in score function.\")\r\n",
    "flags.DEFINE_string(\"loss\", \"approx_ndcg_loss\",\r\n",
    "                    \"The RankingLossKey for the loss function.\")\r\n",
    "flags.DEFINE_string(\"weights_feature_name\", \"\",\r\n",
    "                    \"The name of the feature where unbiased learning-to-rank \"\r\n",
    "                    \"weights are stored.\")\r\n",
    "flags.DEFINE_bool(\"listwise_inference\", False,\r\n",
    "                  \"If true, exports accept `data_format` while serving.\")\r\n",
    "flags.DEFINE_bool(\r\n",
    "    \"use_document_interaction\", False,\r\n",
    "    \"If True, use Document Interaction Network to capture cross-document \"\r\n",
    "    \"interactions as additional features for scoring.\")\r\n",
    "flags.DEFINE_integer(\r\n",
    "    \"num_attention_layers\", 1, \"number of attention layers. See \"\r\n",
    "    \"`tfr.keras.layers.DocumentInteractionAttention`.\")\r\n",
    "flags.DEFINE_integer(\r\n",
    "    \"num_attention_heads\", 1, \"number of self attention heads. See \"\r\n",
    "    \"`tfr.keras.layers.DocumentInteractionAttention`.\")\r\n",
    "flags.DEFINE_integer(\r\n",
    "    \"head_size\", 128, \"Size of attention head. See \"\r\n",
    "    \"`tfr.keras.layers.DocumentInteractionAttention`.\")\r\n",
    "\r\n",
    "FLAGS = flags.FLAGS\r\n",
    "\r\n",
    "_LABEL_FEATURE = \"relevance\"\r\n",
    "_PADDING_LABEL = -1\r\n",
    "_EMBEDDING_DIMENSION = 20\r\n",
    "_MASK = \"mask\"\r\n",
    "\r\n",
    "\r\n",
    "def context_feature_columns():\r\n",
    "  \"\"\"Returns context feature names to column definitions.\"\"\"\r\n",
    "  if FLAGS.vocab_path:\r\n",
    "    sparse_column = tf.feature_column.categorical_column_with_vocabulary_file(\r\n",
    "        key=\"query_tokens\", vocabulary_file=FLAGS.vocab_path)\r\n",
    "  else:\r\n",
    "    sparse_column = tf.feature_column.categorical_column_with_hash_bucket(\r\n",
    "        key=\"query_tokens\", hash_bucket_size=100)\r\n",
    "  query_embedding_column = tf.feature_column.embedding_column(\r\n",
    "      sparse_column, _EMBEDDING_DIMENSION)\r\n",
    "  return {\"query_tokens\": query_embedding_column}\r\n",
    "\r\n",
    "\r\n",
    "def example_feature_columns(use_weight_feature=True):\r\n",
    "  \"\"\"Returns the example feature columns.\"\"\"\r\n",
    "  if FLAGS.vocab_path:\r\n",
    "    sparse_column = tf.feature_column.categorical_column_with_vocabulary_file(\r\n",
    "        key=\"document_tokens\", vocabulary_file=FLAGS.vocab_path)\r\n",
    "  else:\r\n",
    "    sparse_column = tf.feature_column.categorical_column_with_hash_bucket(\r\n",
    "        key=\"document_tokens\", hash_bucket_size=100)\r\n",
    "  document_embedding_column = tf.feature_column.embedding_column(\r\n",
    "      sparse_column, _EMBEDDING_DIMENSION)\r\n",
    "  feature_columns = {\"document_tokens\": document_embedding_column}\r\n",
    "  if use_weight_feature and FLAGS.weights_feature_name:\r\n",
    "    feature_columns[FLAGS.weights_feature_name] = (\r\n",
    "        tf.feature_column.numeric_column(FLAGS.weights_feature_name,\r\n",
    "                                         default_value=1.))\r\n",
    "  return feature_columns\r\n",
    "\r\n",
    "\r\n",
    "def make_input_fn(file_pattern,\r\n",
    "                  batch_size,\r\n",
    "                  randomize_input=True,\r\n",
    "                  num_epochs=None):\r\n",
    "  \"\"\"Returns `Estimator` `input_fn` for TRAIN and EVAL.\r\n",
    "  Args:\r\n",
    "    file_pattern: (string) file pattern for the TFRecord input data.\r\n",
    "    batch_size: (int) number of input examples to process per batch.\r\n",
    "    randomize_input: (bool) if true, randomize input example order. It should\r\n",
    "      almost always be true except for unittest/debug purposes.\r\n",
    "    num_epochs: (int) Number of times the input dataset must be repeated. None\r\n",
    "      to repeat the data indefinitely.\r\n",
    "  Returns:\r\n",
    "    An `input_fn` for `Estimator`.\r\n",
    "  \"\"\"\r\n",
    "  tf.compat.v1.logging.info(\"FLAGS.data_format={}\".format(FLAGS.data_format))\r\n",
    "\r\n",
    "  def _input_fn():\r\n",
    "    \"\"\"Defines the input_fn.\"\"\"\r\n",
    "    context_feature_spec = tf.feature_column.make_parse_example_spec(\r\n",
    "        context_feature_columns().values())\r\n",
    "    label_column = tf.feature_column.numeric_column(\r\n",
    "        _LABEL_FEATURE, dtype=tf.int64, default_value=_PADDING_LABEL)\r\n",
    "    example_feature_spec = tf.feature_column.make_parse_example_spec(\r\n",
    "        list(example_feature_columns().values()) + [label_column])\r\n",
    "    dataset = tfr.data.build_ranking_dataset(\r\n",
    "        file_pattern=file_pattern,\r\n",
    "        data_format=FLAGS.data_format,\r\n",
    "        batch_size=batch_size,\r\n",
    "        list_size=FLAGS.list_size,\r\n",
    "        context_feature_spec=context_feature_spec,\r\n",
    "        example_feature_spec=example_feature_spec,\r\n",
    "        reader=tf.data.TFRecordDataset,\r\n",
    "        shuffle=randomize_input,\r\n",
    "        num_epochs=num_epochs,\r\n",
    "        mask_feature_name=_MASK)\r\n",
    "    features = tf.compat.v1.data.make_one_shot_iterator(dataset).get_next()\r\n",
    "    label = tf.squeeze(features.pop(_LABEL_FEATURE), axis=2)\r\n",
    "    label = tf.cast(label, tf.float32)\r\n",
    "\r\n",
    "    return features, label\r\n",
    "\r\n",
    "  return _input_fn\r\n",
    "\r\n",
    "\r\n",
    "def make_serving_input_fn():\r\n",
    "  \"\"\"Returns serving input fn.\"\"\"\r\n",
    "  context_feature_spec = tf.feature_column.make_parse_example_spec(\r\n",
    "      context_feature_columns().values())\r\n",
    "  example_feature_spec = tf.feature_column.make_parse_example_spec(\r\n",
    "      example_feature_columns().values())\r\n",
    "  if FLAGS.listwise_inference:\r\n",
    "    # Exports accept the specified FLAGS.data_format during serving.\r\n",
    "    return tfr.data.build_ranking_serving_input_receiver_fn(\r\n",
    "        data_format=FLAGS.data_format,\r\n",
    "        context_feature_spec=context_feature_spec,\r\n",
    "        example_feature_spec=example_feature_spec,\r\n",
    "        mask_feature_name=_MASK)\r\n",
    "  elif FLAGS.group_size == 1:\r\n",
    "    # Exports accept tf.Example when group_size = 1.\r\n",
    "    feature_spec = {}\r\n",
    "    feature_spec.update(example_feature_spec)\r\n",
    "    feature_spec.update(context_feature_spec)\r\n",
    "    return tf.estimator.export.build_parsing_serving_input_receiver_fn(\r\n",
    "        feature_spec)\r\n",
    "  else:\r\n",
    "    raise ValueError(\"FLAGS.group_size should be 1, but is {} when \"\r\n",
    "                     \"FLAGS.export_listwise_inference is False\".format(\r\n",
    "                         FLAGS.group_size))\r\n",
    "\r\n",
    "\r\n",
    "def make_transform_fn():\r\n",
    "  \"\"\"Returns a transform_fn that converts features to dense Tensors.\"\"\"\r\n",
    "\r\n",
    "  def _transform_fn(features, mode):\r\n",
    "    \"\"\"Defines transform_fn.\"\"\"\r\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT and not FLAGS.listwise_inference:\r\n",
    "      # We expect tf.Example as input during serving. In this case, group_size\r\n",
    "      # must be set to 1.\r\n",
    "      if FLAGS.group_size != 1:\r\n",
    "        raise ValueError(\r\n",
    "            \"group_size should be 1 to be able to export model, but get %s\" %\r\n",
    "            FLAGS.group_size)\r\n",
    "      context_features, example_features = (\r\n",
    "          tfr.feature.encode_pointwise_features(\r\n",
    "              features=features,\r\n",
    "              context_feature_columns=context_feature_columns(),\r\n",
    "              example_feature_columns=example_feature_columns(),\r\n",
    "              mode=mode,\r\n",
    "              scope=\"transform_layer\"))\r\n",
    "    else:\r\n",
    "      mask = features.pop(_MASK)\r\n",
    "      context_features, example_features = tfr.feature.encode_listwise_features(\r\n",
    "          features=features,\r\n",
    "          context_feature_columns=context_feature_columns(),\r\n",
    "          example_feature_columns=example_feature_columns(),\r\n",
    "          mode=mode,\r\n",
    "          scope=\"transform_layer\")\r\n",
    "\r\n",
    "      # Document interaction attention layer.\r\n",
    "      if FLAGS.use_document_interaction:\r\n",
    "        training = (mode == tf.estimator.ModeKeys.TRAIN)\r\n",
    "        concat_tensor = tfr.keras.layers.ConcatFeatures()(\r\n",
    "            inputs=(context_features, example_features, mask))\r\n",
    "        din_layer = tfr.keras.layers.DocumentInteractionAttention(\r\n",
    "            num_heads=FLAGS.num_attention_heads,\r\n",
    "            head_size=FLAGS.head_size,\r\n",
    "            num_layers=FLAGS.num_attention_layers,\r\n",
    "            dropout=FLAGS.dropout_rate)\r\n",
    "        example_features[\"document_interaction_embedding\"] = din_layer(\r\n",
    "            inputs=(concat_tensor, mask), training=training)\r\n",
    "\r\n",
    "    return context_features, example_features\r\n",
    "\r\n",
    "  return _transform_fn\r\n",
    "\r\n",
    "\r\n",
    "def make_score_fn():\r\n",
    "  \"\"\"Returns a scoring function to build `EstimatorSpec`.\"\"\"\r\n",
    "\r\n",
    "  def _score_fn(context_features, group_features, mode, params, config):\r\n",
    "    \"\"\"Defines the network to score a group of documents.\"\"\"\r\n",
    "    del [params, config]\r\n",
    "    with tf.compat.v1.name_scope(\"input_layer\"):\r\n",
    "      context_input = [\r\n",
    "          tf.compat.v1.layers.flatten(context_features[name])\r\n",
    "          for name in sorted(context_feature_columns())\r\n",
    "      ]\r\n",
    "      group_input = [\r\n",
    "          tf.compat.v1.layers.flatten(group_features[name])\r\n",
    "          for name in sorted(example_feature_columns(use_weight_feature=False))\r\n",
    "      ]\r\n",
    "      if FLAGS.use_document_interaction:\r\n",
    "        group_input.append(\r\n",
    "            tf.compat.v1.layers.flatten(\r\n",
    "                group_features[\"document_interaction_embedding\"]))\r\n",
    "      input_layer = tf.concat(context_input + group_input, 1)\r\n",
    "      tf.compat.v1.summary.scalar(\"input_sparsity\",\r\n",
    "                                  tf.nn.zero_fraction(input_layer))\r\n",
    "      tf.compat.v1.summary.scalar(\"input_max\",\r\n",
    "                                  tf.reduce_max(input_tensor=input_layer))\r\n",
    "      tf.compat.v1.summary.scalar(\"input_min\",\r\n",
    "                                  tf.reduce_min(input_tensor=input_layer))\r\n",
    "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\r\n",
    "    cur_layer = input_layer\r\n",
    "    cur_layer = tf.compat.v1.layers.batch_normalization(\r\n",
    "        cur_layer, training=is_training, momentum=0.99)\r\n",
    "\r\n",
    "    for i, layer_width in enumerate(int(d) for d in FLAGS.hidden_layer_dims):\r\n",
    "      cur_layer = tf.compat.v1.layers.dense(cur_layer, units=layer_width)\r\n",
    "      cur_layer = tf.compat.v1.layers.batch_normalization(\r\n",
    "          cur_layer, training=is_training, momentum=0.99)\r\n",
    "      cur_layer = tf.nn.relu(cur_layer)\r\n",
    "      tf.compat.v1.summary.scalar(\"fully_connected_{}_sparsity\".format(i),\r\n",
    "                                  tf.nn.zero_fraction(cur_layer))\r\n",
    "      cur_layer = tf.compat.v1.layers.dropout(\r\n",
    "          inputs=cur_layer, rate=FLAGS.dropout_rate, training=is_training)\r\n",
    "    logits = tf.compat.v1.layers.dense(cur_layer, units=FLAGS.group_size)\r\n",
    "    return logits\r\n",
    "\r\n",
    "  return _score_fn\r\n",
    "\r\n",
    "\r\n",
    "def eval_metric_fns():\r\n",
    "  \"\"\"Returns a dict from name to metric functions.\"\"\"\r\n",
    "  metric_fns = {}\r\n",
    "  metric_fns.update({\r\n",
    "      \"metric/%s\" % name: tfr.metrics.make_ranking_metric_fn(name) for name in [\r\n",
    "          tfr.metrics.RankingMetricKey.ARP,\r\n",
    "          tfr.metrics.RankingMetricKey.ORDERED_PAIR_ACCURACY,\r\n",
    "      ]\r\n",
    "  })\r\n",
    "  metric_fns.update({\r\n",
    "      \"metric/ndcg@%d\" % topn: tfr.metrics.make_ranking_metric_fn(\r\n",
    "          tfr.metrics.RankingMetricKey.NDCG, topn=topn)\r\n",
    "      for topn in [1, 3, 5, 10]\r\n",
    "  })\r\n",
    "  for topn in [1, 3, 5, 10]:\r\n",
    "    metric_fns[\"metric/weighted_ndcg@%d\" % topn] = (\r\n",
    "        tfr.metrics.make_ranking_metric_fn(\r\n",
    "            tfr.metrics.RankingMetricKey.NDCG,\r\n",
    "            weights_feature_name=FLAGS.weights_feature_name, topn=topn))\r\n",
    "  return metric_fns\r\n",
    "\r\n",
    "\r\n",
    "def train_and_eval():\r\n",
    "  \"\"\"Train and Evaluate.\"\"\"\r\n",
    "  train_input_fn = make_input_fn(FLAGS.train_path, FLAGS.batch_size)\r\n",
    "  eval_input_fn = make_input_fn(\r\n",
    "      FLAGS.eval_path, FLAGS.batch_size, randomize_input=False, num_epochs=1)\r\n",
    "\r\n",
    "  optimizer = tf.compat.v1.train.AdagradOptimizer(\r\n",
    "      learning_rate=FLAGS.learning_rate)\r\n",
    "\r\n",
    "  def _train_op_fn(loss):\r\n",
    "    \"\"\"Defines train op used in ranking head.\"\"\"\r\n",
    "    update_ops = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.UPDATE_OPS)\r\n",
    "    minimize_op = optimizer.minimize(\r\n",
    "        loss=loss, global_step=tf.compat.v1.train.get_global_step())\r\n",
    "    train_op = tf.group([minimize_op, update_ops])\r\n",
    "    return train_op\r\n",
    "\r\n",
    "  ranking_head = tfr.head.create_ranking_head(\r\n",
    "      loss_fn=tfr.losses.make_loss_fn(\r\n",
    "          FLAGS.loss,\r\n",
    "          weights_feature_name=FLAGS.weights_feature_name),\r\n",
    "      eval_metric_fns=eval_metric_fns(),\r\n",
    "      train_op_fn=_train_op_fn)\r\n",
    "\r\n",
    "  estimator = tf.estimator.Estimator(\r\n",
    "      model_fn=tfr.model.make_groupwise_ranking_fn(\r\n",
    "          group_score_fn=make_score_fn(),\r\n",
    "          group_size=FLAGS.group_size,\r\n",
    "          transform_fn=make_transform_fn(),\r\n",
    "          ranking_head=ranking_head),\r\n",
    "      model_dir=FLAGS.model_dir,\r\n",
    "      config=tf.estimator.RunConfig(save_checkpoints_steps=1000))\r\n",
    "\r\n",
    "  train_spec = tf.estimator.TrainSpec(\r\n",
    "      input_fn=train_input_fn, max_steps=FLAGS.num_train_steps)\r\n",
    "\r\n",
    "  exporters = tf.estimator.LatestExporter(\r\n",
    "      \"saved_model_exporter\", serving_input_receiver_fn=make_serving_input_fn())\r\n",
    "\r\n",
    "  eval_spec = tf.estimator.EvalSpec(\r\n",
    "      name=\"eval\",\r\n",
    "      input_fn=eval_input_fn,\r\n",
    "      steps=1,\r\n",
    "      exporters=exporters,\r\n",
    "      start_delay_secs=0,\r\n",
    "      throttle_secs=15)\r\n",
    "\r\n",
    "  # Train and validate.\r\n",
    "  tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\r\n",
    "\r\n",
    "\r\n",
    "def main(_):\r\n",
    "  tf.compat.v1.set_random_seed(1234)\r\n",
    "  tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\r\n",
    "  if FLAGS.use_document_interaction and not FLAGS.listwise_inference:\r\n",
    "    raise ValueError(\"Only listwise inference is compatible for models \"\r\n",
    "                     \"using Document Interaction Network.\")\r\n",
    "  train_and_eval()\r\n",
    "\r\n",
    "\r\n",
    "if __name__ == \"__main__\":\r\n",
    "  flags.mark_flag_as_required(\"train_path\")\r\n",
    "  flags.mark_flag_as_required(\"eval_path\")\r\n",
    "  flags.mark_flag_as_required(\"model_dir\")\r\n",
    "  tf.compat.v1.app.run()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e2b6eb828a06aba2a5f38c98b0f742e720c3c24745e1f8335692b0a3c2eb5e5e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
