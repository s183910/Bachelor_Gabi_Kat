{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages and libraries\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from numpy import absolute\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def func_xg(df, model_name):\n",
    "    tom_res = pd.DataFrame(columns = ['not_4_use','Idx','Gender_bins','Preds'])\n",
    "    for i in range(1):\n",
    "        df = df.copy()\n",
    "\n",
    "        if model_name == 'WO':\n",
    "            X = df.drop(['COL_GRADE_AVG','Unnamed: 0','CR_S11','CC_S11','ENG_S11','CR_PRO','CC_PRO','ENG_PRO', 'STRATUM','GENDER','SCHOOL_NAT',\t'SCHOOL_TYPE',\t'MAT_S11'\t,'BIO_S11'], axis=1).copy()\n",
    "            y=df['COL_GRADE_AVG'].copy()\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=i, test_size=0.33)#, stratify=y)\n",
    "            for_later_gender = X_test.drop(['HI_GRADE_AVG','QR_PRO', 'WC_PRO'], axis = 1)\n",
    "            X_train = X_train.drop('GENDER_bin', axis = 1)\n",
    "            X_test = X_test.drop('GENDER_bin', axis = 1)\n",
    "            gender = for_later_gender.to_numpy()\n",
    "        \n",
    "        else:\n",
    "            X = df.drop(['COL_GRADE_AVG','GENDER','Unnamed: 0','CR_PRO','CC_PRO','ENG_PRO'], axis=1).copy()                \n",
    "            y=df['COL_GRADE_AVG'].copy()\n",
    "            # for_later_gender = X_test.drop(['HI_GRADE_AVG','QR_PRO',\t'WC_PRO'], axis = 1)\n",
    "            \n",
    "            X_encoded = pd.get_dummies(X,columns=['STRATUM','SCHOOL_TYPE','SCHOOL_NAT'])\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X_encoded,y,random_state=i, test_size=0.33)#, stratify=y)\n",
    "            gender = X_test.GENDER_bin.to_numpy()\n",
    "      \n",
    "        dfs = X_train.copy()\n",
    "        for column in dfs.columns:\n",
    "            try:\n",
    "                dfs[column] = (dfs[column] - dfs[column].mean())/dfs[column].std()  \n",
    "            except:\n",
    "                pass\n",
    "        X_train = dfs\n",
    "        \n",
    "        dft = X_test.copy()\n",
    "        for column in dft.columns:\n",
    "            try:\n",
    "                dft[column] = (dft[column] - dft[column].mean())/dft[column].std()  \n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        X_test = dft\n",
    "\n",
    "        final_model = xgb.XGBRegressor(seed=24, objective='reg:squarederror', max_depth=3, learning_rate=0.05, gamma=0,reg_lambda=15.0, scale_pos_weight=0.01)\n",
    "        final_model.fit(X_train,y_train,verbose=True, eval_metric='rmse')\n",
    "\n",
    "        y_pred_final = final_model.predict(X_test)\n",
    "        \n",
    "        index = X_test.index.to_numpy()\n",
    "    \n",
    "        preds = y_pred_final\n",
    "        listen = list(map(list, zip(index,gender, preds)))\n",
    "\n",
    "        samlet_preds_index_without_sensitive = pd.DataFrame(listen, columns=['Idx','Gender_bins','Preds'])\n",
    "        samlet_preds_index_without_sensitive = samlet_preds_index_without_sensitive.sort_values(by=['Preds'],ascending=False)\n",
    "\n",
    "        samlet_preds_index_without_sensitive = samlet_preds_index_without_sensitive.reset_index()\n",
    "        \n",
    "        # samlet_preds_index_without_sensitive['Gender_bins'] = samlet_preds_index_without_sensitive['Gender_bins'].str.strip('[]').astype(int)\n",
    "        \n",
    "\n",
    "\n",
    "        # result = samlet_preds_index_without_sensitive.to_csv(f\"func_xg_wo_res/samlet_preds_index_without_sensitive_ny{i}_{model_name}.csv\")\n",
    "        tom_res = tom_res.append(samlet_preds_index_without_sensitive)\n",
    "\n",
    "    tom_res = tom_res.drop(['index','not_4_use'], axis=1)\n",
    "    return tom_res.to_csv(f\"func_xg_wo_res/samlet_preds_index_ny_{model_name}.csv\")\n",
    "    # return tom_res.to_csv(f\"Bachelor_Gabi_Kat/fairsearch-fair-python/samlet_preds_index_ny_{model_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"GUDF.csv\")\n",
    "\n",
    "\n",
    "func_xg(df, model_name='WO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "20480 /5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages and libraries\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "# Load data before standardization\n",
    "df = pd.read_csv(\"df_sum_score_ex.csv\")\n",
    "\n",
    "\n",
    "X = df.drop(['COL_GRADE_AVG','GENDER','Unnamed: 0','CR_PRO','CC_PRO','ENG_PRO'], axis=1).copy()\n",
    "\n",
    "y=df['COL_GRADE_AVG'].copy()\n",
    "\n",
    "\n",
    "X_encoded = pd.get_dummies(X,columns=['STRATUM','SCHOOL_TYPE','SCHOOL_NAT'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded,y,random_state=24, test_size=0.33)#, stratify=y)\n",
    "\n",
    "\n",
    "#apply standardization to filtered original dataframe\n",
    "dfs = X_train.copy()\n",
    "for column in dfs.columns:\n",
    "    try:\n",
    "        dfs[column] = (dfs[column] - dfs[column].mean())/dfs[column].std()  \n",
    "    except:\n",
    "        pass\n",
    "\n",
    "X_train = dfs\n",
    "#apply standardization to filtered original dataframe\n",
    "\n",
    "dft = X_test.copy()\n",
    "for column in dft.columns:\n",
    "    try:\n",
    "        dft[column] = (dft[column] - dft[column].mean())/dft[column].std()  \n",
    "    except:\n",
    "        pass\n",
    "\n",
    "X_test = dft\n",
    "# Regressor\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt \n",
    "# XGBRegressor\n",
    "\n",
    "model = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Prediction\n",
    "y_pred_init = model.predict(X_test)\n",
    "y_pred_init\n",
    "# Mean squared error\n",
    "# This is the average squared difference between the estimated values and the actual value.\n",
    "# KÃ¸r disse herunder for at se om det er blevet bedre \n",
    "mse_init = mean_squared_error(y_test, y_pred_init)\n",
    "\n",
    "# Root mean squared error\n",
    "rmse_init= np.sqrt(mse_init)\n",
    "\n",
    "print(\"INITIAL MSE: %.2f\" % mse_init)\n",
    "print(\"INITIAL RMSE: %.2f\" % rmse_init)\n",
    "\n",
    "# # Optimize parameters using cross-validation\n",
    "\n",
    "# # XGBoost has a lot of hyper parameters which we can manually tune. These are e.g. max_depth of the tree, learning_rate, gamma which encourages pruning of the tree and reg_lambda which is the regularization parameter lambda. We are trying to find the optimal values for these parameters and hope that we can improve the accuracy in the prediction of the candidates college grades. \n",
    "# # Gridsearch cross validation \n",
    "\n",
    "# # Gridsearch CV will be run sequentially on subsets of different options for the parameters. \n",
    "\n",
    "# # 1st round\n",
    "# param_grid_1 = {\n",
    "#     'max_depth':[3,4,5],\n",
    "#     'learning_rate':[0.01,0.05,0.1],\n",
    "#     'gamma':[0,0.25,0.1],\n",
    "#     'reg_lambda':[0,0.1,10.0],\n",
    "#     'scale_pos_weight':[1,3,5]\n",
    "# }\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# # base_estimator_model = xgb.XGBRegressor()\n",
    "# # clf = GridSearchCV(base_estimator_model, param_grid)\n",
    "# # clf.fit(X_train, y_train)\n",
    "\n",
    "# optimal_params = GridSearchCV(\n",
    "#     estimator=xgb.XGBRegressor(\n",
    "#         objective='reg:squarederror', seed=24, subsample=0.9),\n",
    "#         param_grid=param_grid_1,\n",
    "#         cv=5).fit(X_train,y_train)\n",
    "          \n",
    "# # optimal parameters \n",
    "# print(optimal_params.best_params_)\n",
    "\n",
    "\n",
    "# ## Round 2 \n",
    "# param_grid_2 = {\n",
    "#     'max_depth':[2,3],\n",
    "#     'learning_rate':[0.05],\n",
    "#     'gamma':[0,0.01],\n",
    "#     'reg_lambda':[10.0,15.0,20.0],\n",
    "#     'scale_pos_weight':[0.01,0.5,1]\n",
    "# }\n",
    "# optimal_params = GridSearchCV(\n",
    "#     estimator=xgb.XGBRegressor(\n",
    "#         objective='reg:squarederror', seed=24, subsample=0.9),\n",
    "#         param_grid=param_grid_2,\n",
    "#         cv=5).fit(X_train,y_train)\n",
    "# # optimal parameters \n",
    "# print(optimal_params.best_params_)\n",
    "\n",
    "\n",
    "# ## Round 3 \n",
    "# param_grid_3 = {\n",
    "#     'max_depth':[3],\n",
    "#     'learning_rate':[0.05],\n",
    "#     'gamma':[0],\n",
    "#     'reg_lambda':[10.0],\n",
    "#     'scale_pos_weight':[1]\n",
    "# }\n",
    "\n",
    "\n",
    "final_model = xgb.XGBRegressor(seed=24, objective='reg:squarederror', max_depth=3, learning_rate=0.05, gamma=0,reg_lambda=10.0, scale_pos_weight=1)\n",
    "final_model.fit(X_train,y_train,verbose=True, eval_metric='rmse')\n",
    "# eval_metric default according to objective\n",
    "from numpy import absolute\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "# define model evaluation method\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate model\n",
    "scores = cross_val_score(final_model, X_train, y_train, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "\n",
    "# force scores to be positive\n",
    "scores = absolute(scores)\n",
    "print('Mean MAE: %.3f (%.3f)' % (scores.mean(), scores.std()) )\n",
    "\n",
    "# define model evaluation method\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate model\n",
    "scores = cross_val_score(model, X_train, y_train, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "\n",
    "# force scores to be positive\n",
    "scores = absolute(scores)\n",
    "print('Mean MAE: %.3f (%.3f)' % (scores.mean(), scores.std()) )\n",
    "\n",
    "# The model is tunes with grid search CV and evaluated using k-fold cross valdiation. The model is \n",
    "# evaluated on the training data and the average mean absolut error across the repeats of 10-fold cross-validtaion is reported. \n",
    "# There is some variation in the magnitude of errors as seen by theMAE. The average difference between \n",
    "# the predicted college garde and the observed college grade was 11.431 points in the grade. \n",
    "\n",
    "scores_final = cross_val_score(final_model, X_train, y_train, cv=10)\n",
    "print(\"Mean cross-validation score: %.2f\" % scores_final.mean())\n",
    "\n",
    "scores = cross_val_score(model, X_train, y_train, cv=10)\n",
    "print(\"Mean cross-validation score: %.2f\" % scores.mean())\n",
    "y_pred_final = final_model.predict(X_test)\n",
    "y_pred_final\n",
    "# KÃ¸r disse herunder for at se om det er blevet bedre \n",
    "mse_final = mean_squared_error(y_test, y_pred_final)\n",
    "print(\"FINAL MSE: %.2f\" % mse_final)\n",
    "\n",
    "# Root mean squared error\n",
    "rmse_final= np.sqrt(mse_final)\n",
    "print(\"FINAL RMSE: %.2f\" % rmse_final)\n",
    "\n",
    "# from sklearn.metrics import average_precision_score\n",
    "# map_final= average_precision_score(y_test, y_pred_final)\n",
    "# print(\"FINAL MAP: %.2f\" % map_final)\n",
    "\n",
    "\n",
    "# Results\n",
    "FOR RAW DATA VANILLA \n",
    "Before and after parameter optimization\n",
    "- MSE: 219.97  (Ã¦ndring pÃ¥ -6,44%)\n",
    "- FINAL MSE: 205.79\n",
    "\n",
    "FOR RAW DATA WITHOUT SENSITIVE \n",
    "- INITIAL MSE: 207.27\n",
    "- FINAL MSE: 205.79\n",
    "\n",
    "FOR STANDARDIZED DATA \n",
    "- INITIAL MSE: 0.42  (Ã¦ndring pÃ¥ -4,76%)\n",
    "- FINAL MSE: 0.40\n",
    "\n",
    "- FINAL MSE: 0.40\n",
    "- FINAL RMSE: 0.63\n",
    "\n",
    "EX standardized data\n",
    "\n",
    "Mean MAE: 11.055 (0.244)\n",
    "Mean MAE: 11.703 (0.304)\n",
    "Mean cross-validation score: 0.62\n",
    "Mean cross-validation score: 0.57\n",
    "FINAL MSE: 197.41\n",
    "FINAL RMSE: 14.05\n",
    "\n",
    "\n",
    "# plot decision tree\n",
    "\n",
    "from xgboost import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "\n",
    "\n",
    "plot_tree(final_model,fontsize=10)\n",
    "# plt.show()\n",
    "plt.savefig('tree_high_dpi_ex', dpi=400)\n",
    "\n",
    "\n",
    "# Reparring the index and predictions \n",
    "index = X_test.index.to_numpy()\n",
    "gender = X_test.GENDER_bin.to_numpy()\n",
    "gender[:7]\n",
    "\n",
    "\n",
    "preds = y_pred_final # her har de stadig samme rÃ¦kkefÃ¸lge i forhold til index som til at starte med, de er nemlig ikke blevet sorteret endnu\n",
    "listen = list(map(list, zip(index,gender, preds)))\n",
    "\n",
    "for_later_gender = gender\n",
    "for_later_gender = np.save('for_later_gender.npy', for_later_gender)    # .npy extension is added if not given\n",
    "\n",
    "\n",
    "samlet_preds_plain_ex = pd.DataFrame(listen, columns=['Idx','Gender_bins','Preds'])\n",
    "\n",
    "\n",
    "samlet_preds_plain_ex = samlet_preds_plain_ex.sort_values(by=['Preds'],ascending=False)\n",
    "samlet_preds_plain_ex.to_csv(\"samlet_preds_plain_ex.csv\")\n",
    "samlet_preds_plain_ex \n",
    "# df = pd.read_csv(\"df_sum_score_ex.csv\")\n",
    "# dfs = df.copy()\n",
    "# mean_col = []\n",
    "# std_col = []\n",
    "# col = []\n",
    "# for column in dfs.columns:\n",
    "#     try: \n",
    "#         mean_col.append(dfs[column].mean())\n",
    "#         col.append(column)\n",
    "#         std_col.append(dfs[column].std())\n",
    "#     except:\n",
    "#         pass \n",
    "\n",
    "# raw_mean_std = pd.DataFrame(\n",
    "#     {'mean': mean_col,\n",
    "#      'std': std_col,\n",
    "#      'col': col\n",
    "#     })\n",
    "\n",
    "# #apply standardization to filtered original dataframe\n",
    "# dfs_rev = samlet_preds_plain_ex.copy()\n",
    "# dfs_rev\n",
    "# sorte predictions med mergesort \n",
    "import numpy as np\n",
    "\n",
    "pred_sorted = np.sort(y_pred_final, axis=- 1, kind='mergesort')[::-1]\n",
    "pred_sorted\n",
    "\n",
    "\n",
    "\n",
    "# Plotting\n",
    "import matplotlib as mpl\n",
    "def setup_mpl():\n",
    "    mpl.rcParams[\"font.family\"] = \"Helvetica Neue\"\n",
    "    mpl.rcParams[\"font.size\"] = 11\n",
    "    mpl.rcParams[\"figure.figsize\"] = (10,5)\n",
    "    mpl.rcParams[\"figure.dpi\"] = 200\n",
    "    # mpl.rcParams[\"lines.linewidth\"] = 1\n",
    "setup_mpl()\n",
    "\n",
    "x_ax = range(len(y_test))\n",
    "y_test_sorted = np.sort(y_test, axis=- 1, kind='mergesort')[::-1]\n",
    "plt.plot(x_ax, y_test_sorted, label=\"original\")\n",
    "plt.plot(x_ax, pred_sorted, label=\"predicted\")\n",
    "plt.title(\"College admissions test and predicted data\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig(\"plot_xgb_plain_ex.png\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e2b6eb828a06aba2a5f38c98b0f742e720c3c24745e1f8335692b0a3c2eb5e5e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
